{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_It9C2TJ05K"
   },
   "source": [
    "# Mathematical Formulation for IMDB Text Classification using an MLP\n",
    "# Instructor: Dr. Ankur Mali\n",
    "# University of South Florida (Spring 2025)\n",
    "\n",
    "This document describes the mathematical framework for processing IMDB text data using a character-level bag-of-characters representation, passing it through a multi-layer perceptron (MLP), and training the model via gradient descent. The evaluation metrics include loss, accuracy, precision, and recall.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Tokenization and Input Representation\n",
    "\n",
    "Given a raw text review \\( T \\), we first tokenize it at the character level. Let \\( V \\) be the vocabulary (i.e., the set of unique characters) extracted from the training data with size \\( |V| = d \\).\n",
    "\n",
    "For each text review \\( T \\), we construct a binary bag-of-characters vector \\( x \\in \\{0,1\\}^d \\) such that:\n",
    "\n",
    "$$\n",
    "x_j =\n",
    "\\begin{cases}\n",
    "1, & \\text{if the } j\\text{-th character in } V \\text{ appears in } T, \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, each review is represented as:\n",
    "\n",
    "$$\n",
    "x = \\mathrm{BOW}(T) \\in \\mathbb{R}^d.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. MLP Model\n",
    "\n",
    "The MLP we consider has the following structure:\n",
    "- **Input layer:** Receives $$( x \\in \\mathbb{R}^d )$$.\n",
    "- **Hidden Layer 1:** With $h_1$ or $z_1$ (Post-activation) neurons.\n",
    "- **Hidden Layer 2:** With \\( h_2 \\) neurons.\n",
    "- **Output Layer:** With \\( c \\) neurons (for \\( c = 2 \\) classes in binary classification).\n",
    "\n",
    "### 2.1. Model Parameters\n",
    "\n",
    "- **First Hidden Layer:**\n",
    "  - Weight matrix: $$(W^{(1)} \\in \\mathbb{R}^{d \\times h_1} )$$\n",
    "  - Bias vector: $$( b^{(1)} \\in \\mathbb{R}^{h_1} )$$\n",
    "\n",
    "- **Second Hidden Layer:**\n",
    "  - Weight matrix: $$( W^{(2)} \\in \\mathbb{R}^{h_1 \\times h_2} )$$\n",
    "  - Bias vector: $$( b^{(2)} \\in \\mathbb{R}^{h_2} )$$\n",
    "\n",
    "- **Output Layer:**\n",
    "  - Weight matrix: $$( W^{(3)} \\in \\mathbb{R}^{h_2 \\times c} )$$\n",
    "  - Bias vector: $$( b^{(3)} \\in \\mathbb{R}^{c} )$$\n",
    "\n",
    "> **Note:** In the original code, a third hidden layer size (\\( h_3 \\)) is provided as a parameter but is not used in the forward computation. Here, the model uses two hidden layers. You can add any N layers, to this pipeline, remember to modify the pipeline accordingly.\n",
    "\n",
    "### 2.2. Forward Pass\n",
    "\n",
    "For an input vector \\( x \\), the forward propagation through the network is as follows:\n",
    "\n",
    "1. **First Hidden Layer:**\n",
    "\n",
    "   $$\n",
    "   h^{(1)} = \\text{ReLU}\\Big( x\\, W^{(1)} + b^{(1)} \\Big)\n",
    "   $$\n",
    "\n",
    "2. **Second Hidden Layer:**\n",
    "\n",
    "   $$\n",
    "   h^{(2)} = \\text{ReLU}\\Big( h^{(1)}\\, W^{(2)} + b^{(2)} \\Big)\n",
    "   $$\n",
    "\n",
    "3. **Output Layer (Logits):**\n",
    "\n",
    "   $$\n",
    "   z = h^{(2)}\\, W^{(3)} + b^{(3)}\n",
    "   $$\n",
    "\n",
    "The logits \\( z \\) are then converted to class probabilities using the softmax function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z) = \\frac{\\exp(z)}{\\sum_{j=1}^{c} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Loss Function\n",
    "\n",
    "We use the **Categorical Cross Entropy Loss** (with logits) for training. For a single sample with true one-hot label \\( y \\) and predicted probabilities \\( \\hat{y} \\), the loss is:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\sum_{j=1}^{c} y_j \\log(\\hat{y}_j)\n",
    "$$\n",
    "\n",
    "For a batch of \\( N \\) samples, the average loss is computed as:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training via Gradient Descent\n",
    "\n",
    "The goal is to minimize the loss \\( \\mathcal{L} \\) with respect to the model parameters:\n",
    "\n",
    "$$\n",
    "\\Theta = \\{ W^{(1)},\\, b^{(1)},\\, W^{(2)},\\, b^{(2)},\\, W^{(3)},\\, b^{(3)} \\}\n",
    "$$\n",
    "\n",
    "Using gradient descent (or an adaptive method like Adam), each parameter \\( \\theta \\in \\Theta \\) is updated as:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the learning rate.\n",
    "- $\\nabla_\\theta L $ denotes the gradient of the loss with respect to $\\theta $.\n",
    "\n",
    "Backpropagation is used to compute these gradients efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Evaluation Metrics\n",
    "\n",
    "In addition to monitoring the loss during training, we evaluate the model performance using:\n",
    "\n",
    "- **Accuracy:**\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "  $$\n",
    "\n",
    "- **Precision:**\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "\n",
    "- **Recall:**\n",
    "\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "These metrics are computed on the validation and test sets to assess the model’s generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary of the Pipeline\n",
    "\n",
    "1. **Tokenization:**  \n",
    "   Each review \\( T \\) is tokenized at the character level and converted into a binary vector $$x \\in \\{0,1\\}^d$$ representing the presence of each character in the vocabulary \\( V \\).\n",
    "\n",
    "2. **MLP Forward Propagation:**  \n",
    "   The input vector \\( x \\) is propagated through the MLP:\n",
    "   - First hidden layer: $$ h^{(1)} = \\text{ReLU}\\big( x\\, W^{(1)} + b^{(1)} \\big) $$\n",
    "   - Second hidden layer: $$ h^{(2)} = \\text{ReLU}\\big( h^{(1)}\\, W^{(2)} + b^{(2)} \\big) $$\n",
    "   - Output layer: $$ z = h^{(2)}\\, W^{(3)} + b^{(3)} $$\n",
    "   - Softmax conversion: $$ \\hat{y} = \\text{softmax}(z) $$\n",
    "\n",
    "3. **Loss Computation:**  \n",
    "   The categorical cross entropy loss L is computed using the true labels and the predicted probabilities.\n",
    "\n",
    "4. **Training:**  \n",
    "   The model parameters $\\Theta$ are updated using gradient descent (or Adam), where:\n",
    "\n",
    "   $$\n",
    "   \\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
    "   $$\n",
    "\n",
    "5. **Evaluation:**  \n",
    "   After training, the model is evaluated on the validation and test sets using the loss, accuracy, precision, and recall metrics.\n",
    "\n",
    "---\n",
    "\n",
    "This formulation captures the entire process—from transforming raw text into a numeric representation, through the forward and backward passes of an MLP, to the training and evaluation of the system. Shorter version of your slides :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T06:44:54.431817Z",
     "iopub.status.busy": "2025-02-19T06:44:54.431446Z",
     "iopub.status.idle": "2025-02-19T07:03:55.678710Z",
     "shell.execute_reply": "2025-02-19T07:03:55.677811Z",
     "shell.execute_reply.started": "2025-02-19T06:44:54.431782Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/84/76/c55967ac9968ddaede25a4dce37aba37e9030656f02c12676151ce1b6f22/tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow_datasets\n",
      "  Obtaining dependency information for tensorflow_datasets from https://files.pythonhosted.org/packages/43/e6/a85c9a2d8ce72c09c8c8bf231c75e6e12e0e0848df336b28d1ec0beae1fe/tensorflow_datasets-4.9.7-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: scikit-learn in /data/hvaidya/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in /data/hvaidya/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/b8/25/155f9f080d5e4bc0082edfda032ea2bc2b8fab3f4d25d46c1e9dd22a1a89/flatbuffers-25.2.10-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/a3/61/8001b38461d751cd1a0c3a6ae84346796a5758123f3ed97a1b121dfbf4f3/gast-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/a8/45/2ebbde52ad2be18d3675b6bee50e68cd73c9e0654de77d595540b5129df8/protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/7f/be/df630c387a0a054815d60be6a97eb4e8f17385d5d6fe660e1c02750062b4/termcolor-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/9f/e5/5316b239380b8b2ad30373eb5bb25d9fd36c0375e94a98a0a60ea357d254/grpcio-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.19,>=2.18 from https://files.pythonhosted.org/packages/b1/de/021c1d407befb505791764ad2cbd56ceaaa53a746baed01d2e2143f05f18/tensorboard-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/fe/cf/aea9087c4d7fafe956a0cc0ff6c3327d10fb8442cda50f992a2186921fa0/keras-3.8.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Obtaining dependency information for numpy<2.1.0,>=1.26.0 from https://files.pythonhosted.org/packages/ba/a8/c17acf65a931ce551fee11b72e8de63bf7e8a6f0e21add4c937c83563538/numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.11.0 from https://files.pythonhosted.org/packages/03/71/c99f662d4832c8835453cf3476f95daa28372023bda4aa1fca9e97c24f09/h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes<0.5.0,>=0.4.0 from https://files.pythonhosted.org/packages/28/bc/6a2344338ea7b61cd7b46fb24ec459360a5a0903b57c55b156c1e46c644a/ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/66/7f/e36ae148c2f03d61ca1bff24bc13a0fef6d6825c966abef73fc6f880a23b/tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Obtaining dependency information for dm-tree from https://files.pythonhosted.org/packages/e8/46/939fbf81177c7cb3b1e5ddebd696237b3be9520769cce882f064de497103/dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Obtaining dependency information for immutabledict from https://files.pythonhosted.org/packages/59/56/25ca7b848164b7d93dbd5fc97dd7751700c93e324fe854afbeb562ee2f98/immutabledict-4.2.1-py3-none-any.whl.metadata\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow_datasets) (11.0.0)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Obtaining dependency information for simple-parsing from https://files.pythonhosted.org/packages/4f/9c/e9ea38750027a6de3e3c5e68a19fda0e7b0cd3db8045f30d0f6bc113b911/simple_parsing-0.1.7-py3-none-any.whl.metadata\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Obtaining dependency information for tensorflow-metadata from https://files.pythonhosted.org/packages/e0/57/393aa9dde72347cde9e01f665bac344f14adefd5c748be45b23aa5804f6d/tensorflow_metadata-1.16.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: toml in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Collecting array-record>=0.5.0 (from tensorflow_datasets)\n",
      "  Obtaining dependency information for array-record>=0.5.0 from https://files.pythonhosted.org/packages/0c/df/16d19d8aaa61c24896ec94313b271fc16320e402a311bcc54c42ee190a70/array_record-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading array_record-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (692 bytes)\n",
      "Collecting etils[edc,enp,epath,epy,etree]>=1.9.1 (from tensorflow_datasets)\n",
      "  Obtaining dependency information for etils[edc,enp,epath,epy,etree]>=1.9.1 from https://files.pythonhosted.org/packages/05/83/bb4a4518bfa32a160dc455d8d944a6e00a7eb6759f449cb20c7b7879090e/etils-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading etils-1.12.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: fsspec in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (2023.4.0)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1->tensorflow_datasets)\n",
      "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/a4/ed/1f1afb2e9e7f38a545d628f864d562a5ae64fe6f7a10e28ffb9b185b4e89/importlib_resources-6.5.2-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (3.11.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/73/59/7854fbfb59f8ae35483ce93493708be5942ebb6328cd85b3a609df629736/namex-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/b3/9b/b2420d5830d3e65c98543e69dbcebdc903830b897bd601dfab8481fa0b5b/optree-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading optree-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Obtaining dependency information for numpy<2.1.0,>=1.26.0 from https://files.pythonhosted.org/packages/3a/d0/edc009c27b406c4f9cbc79274d6e46d634d139075492ad055e3d68445925/numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from dm-tree->tensorflow_datasets) (22.1.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Obtaining dependency information for docstring-parser<1.0,>=0.15 from https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl.metadata\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Obtaining dependency information for googleapis-common-protos<2,>=1.56.4 from https://files.pythonhosted.org/packages/89/30/2bd0eb03a7dee7727cd2ec643d1e992979e62d5e7443507381cce0455132/googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /data/hvaidya/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading etils-1.12.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=492d2d80f4f7a0f9f92ebdbf7ed60a6631c6c451b3cbde0122218403409e3a40\n",
      "  Stored in directory: /home/h/hvaidya/.cache/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, promise, optree, opt-einsum, numpy, importlib_resources, immutabledict, grpcio, google-pasta, gast, etils, docstring-parser, astunparse, absl-py, tensorboard, simple-parsing, rich, ml-dtypes, h5py, googleapis-common-protos, dm-tree, tensorflow-metadata, keras, tensorflow, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 array-record-0.6.0 astunparse-1.6.3 dm-tree-0.1.9 docstring-parser-0.16 etils-1.12.0 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 googleapis-common-protos-1.67.0 grpcio-1.70.0 h5py-3.13.0 immutabledict-4.2.1 importlib_resources-6.5.2 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.14.0 promise-2.3 protobuf-5.29.3 rich-13.9.4 simple-parsing-0.1.7 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.16.1 tensorflow_datasets-4.9.7 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow_datasets scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-19T07:16:53.026027Z",
     "iopub.status.busy": "2025-02-19T07:16:53.025760Z",
     "iopub.status.idle": "2025-02-19T07:16:53.620309Z",
     "shell.execute_reply": "2025-02-19T07:16:53.619714Z",
     "shell.execute_reply.started": "2025-02-19T07:16:53.026004Z"
    },
    "id": "ad5HQaMorFuu",
    "outputId": "d7b126e4-6872-405c-f188-5bdcc247ee4b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "  File \"/tmp/ipykernel_22745/3879283748.py\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "ModuleNotFoundError: No module named 'tensorflow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2170, in showtraceback\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
      "  File \"/home/hvaidya/miniconda3/envs/tf/lib/python3.10/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "# -------------------------------\n",
    "# MLP Class Definition (look at slides)\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (Note: Not used in compute_output in this example)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device is decided automatically.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden1 = size_hidden1\n",
    "        self.size_hidden2 = size_hidden2\n",
    "        self.size_hidden3 = size_hidden3  # (Currently not used)\n",
    "        self.size_output = size_output\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases for first hidden layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights and biases for second hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        return grads\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # First hidden layer\n",
    "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        z1 = tf.nn.relu(h1)\n",
    "        # Second hidden layer\n",
    "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
    "        z2 = tf.nn.relu(h2)\n",
    "        # Output layer (logits)\n",
    "        output = tf.matmul(z2, self.W3) + self.b3\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def char_level_tokenizer(texts, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts (e.g., movie reviews).\n",
    "        num_words (int or None): Maximum number of tokens to keep (based on frequency).\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing binary presence of characters.\n",
    "    \"\"\"\n",
    "    # Use texts_to_matrix with mode 'binary' to create fixed-length vectors.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage for IMDB Classification\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example IMDB reviews (In practice, load your dataset here)\n",
    "    texts = [\n",
    "        \"I loved this movie! It was fantastic.\",\n",
    "        \"The film was terrible and boring.\"\n",
    "    ]\n",
    "    # One-hot encoded labels for 2 classes (e.g., positive: [0,1], negative: [1,0])\n",
    "    labels = np.array([[0, 1], [1, 0]])\n",
    "\n",
    "    # Create and fit a character-level tokenizer\n",
    "    tokenizer = char_level_tokenizer(texts)\n",
    "\n",
    "    # Convert texts to bag-of-characters representation\n",
    "    X = texts_to_bow(tokenizer, texts)\n",
    "    print(\"Input shape:\", X.shape)\n",
    "\n",
    "    # Set model hyperparameters.\n",
    "    # The input size is equal to the dimension of the bag-of-characters vector.\n",
    "    size_input = X.shape[1]\n",
    "    size_hidden1 = 64\n",
    "    size_hidden2 = 32\n",
    "    size_hidden3 = 16  # Not used in compute_output (placeholder for a potential extra layer)\n",
    "    size_output = 2\n",
    "\n",
    "    # Instantiate the MLP model.\n",
    "    model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
    "\n",
    "    # Define an optimizer.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    # Training loop (for demonstration purposes; adjust epochs and batch size as needed)\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predictions\n",
    "        predictions = model.forward(X)\n",
    "        # Compute loss\n",
    "        current_loss = model.loss(predictions, labels)\n",
    "        # Backward pass: compute gradients\n",
    "        grads = model.backward(X, labels)\n",
    "        # Update weights manually using the optimizer.\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        print(f\"Epoch {epoch+1}, Loss: {current_loss.numpy()}\")\n",
    "\n",
    "    # Testing the model on a new review.\n",
    "    new_text = [\"An amazing film with a thrilling plot.\"]\n",
    "    X_new = texts_to_bow(tokenizer, new_text)\n",
    "    logits = model.forward(X_new)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    print(\"Predicted probabilities:\", probabilities.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNGpqID8UZaW"
   },
   "source": [
    "## MLP on IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476,
     "referenced_widgets": [
      "cb2ca4b8eae746b6990b7a11c90db810",
      "df2756a1e35540abb553129c3b904709",
      "fb9fd2e2a9be4d0d98b6f796cd5f830a",
      "3ebcc18d220745338c4ec8de6127e48e",
      "caf08b27977a4c188e92ea34166acfc8",
      "f60316df87ba4b64bef0b47f95e8d30f",
      "f212087a14d84bd7a5a91d14775a75eb",
      "a85a9cfd1b254b7b98ce7a81661b96ed",
      "ec25dadc702141049913b810b7fde1c7",
      "7ec9320a7eb74e4c98b230fba6d8a2ce",
      "21b3131b402b4b14991f046d9a3a8be0",
      "20a84906121842f48543c1bad206d799",
      "cb85ef2782754246b66058f75317feed",
      "5138ab3ff1dc4cc883a434e1a7caa14a",
      "a7ea683bf59f4687bd3eadba6298a883",
      "d074fc35f4864f55a1e4ebdca6e07530",
      "444ec309e9cc4828b3ae669d28901abc",
      "b594fbc34d5d40d4abae1759c81972af",
      "250a431c05cb4878a428cf415f702b36",
      "e8c785ed43d84e4e856476928b2a49e9",
      "677c67b52ca14e2d88114d167d065797",
      "3db2d29d0c1649218f2f2c604993727a",
      "77a48de94d2948b0ae340c92000cf7e8",
      "3bdad631a663468e947008ba9a952299",
      "9610d42f53d441c8940e894954e11339",
      "b36275d5ce954d7ab7b442bb60729146",
      "aad3c5ae005a4124acd9285769a215e1",
      "bb323561a3de4b8f85a43a075d00119e",
      "d704ab225ada4134bf1ce4bb830a6354",
      "1a10d0b1fd154491a8888340c8ebfe38",
      "85cfedddc7914481a99c93ae7f349851",
      "40845adffae1455689ad5a1559559fa0",
      "35d8ba6f21ab445e856039f4c75082b1",
      "bd5890b2ca084272990e5e16f6b81d8c",
      "f3cbd7d3127f4cefb81665e7f7d7c004",
      "40d486058c5d47a38ac3adb0cf2bc6d2",
      "5b130b325efa49f99be9b52994686a0c",
      "8f11f250de6448ad87a3363ffcb4e243",
      "b71a01539a0f4b0c89bf87eba3780fcc",
      "816a5390057d4567915bf34ad0b54c41",
      "40019e582a064de589847a3da1175eca",
      "7eb9b35910a64aecaa958983ea078dad",
      "e9b7a63ccc5f462abb99fa20caa65cc4",
      "0e66320306b44c65bce10ade0c4d2eeb",
      "88bf3f3840dd4be894756fc2474b84ed",
      "4b056533e06a48a5a81d15cf44b961b7",
      "a4903235f0f0406d9c0bee0678bf8805",
      "3dec77d473c84931836cfa6a0db79ecb",
      "62010f0361854b66afd987c8bd7e15f5",
      "a8e3a8be14b742209b8ef74200a2cd9f",
      "1853768c4e554ae0b1ade9b6e62d771b",
      "43b6dbdb5b6a42d198d384ee7fa4dd29",
      "259e6840f71c4a29b64725a4d921965f",
      "a6ea7ea17fe443ed816de452ba64b2f1",
      "5af6d5859b724a3ba77f579df40b71d4",
      "9fc72e5be2694018a37b7861ccea3ade",
      "2f96003dea1542f58a9ff716fe8713ee",
      "79cd58e72bd4406698ad3979a6c9ba6d",
      "ad10809f5e9045d9905cd083bb346f21",
      "0ee8c01d75c541218b8bc09ff1f94cde",
      "94a33644645a4c27a8b781908bb5c92b",
      "c96784cc747c4dd6b95621289526af8a",
      "25795eca2fe34d14845d14f3295a9ec2",
      "d0a178cf8e0d4bbdb4b521a693ccadbe",
      "98a5b8121fe341299996d32f52261fe6",
      "1547a0be80f84b32a254aea474d6867a",
      "5e72ea42a5714ba5bf783e99af1ffe8b",
      "b411ae301a9c4cac9336c8f1bb18c135",
      "ca846e4a00d5439b89615b29b94dd268",
      "be89b15c9d9e48dda2b0819689faa365",
      "e0716ea457b54fa58b4eef100fab9335",
      "3c9c79a8361a48ea9128f738e67941b4",
      "12dc246bd54c4e3ea3747c65c2e17014",
      "fa3b427945774e91bbaf99d8cbdaf10d",
      "b379d846511c4e17b6ca15c1f6662aae",
      "d91ce7676d13401fb58641aa4e71602c",
      "a614a6f07a7e42baaea992036e16612e",
      "da94d8894e6b4feabcd3d191ffbde635",
      "825e86dc8b6046a0b60e728a911f99f9",
      "26b2ec9c903547f886effcacfeea8629",
      "e19bf5cbd5ce4377939f080e7aafa25f",
      "7df5f5059e5f49c9968599942999833b",
      "708aac86918f45aaa50940f7122b36ab",
      "46836bd062e74b14a9e5ae1f1d48a184",
      "e014cd8004684dd0bf2513e8dcb9e9f8",
      "ff5c2c87b086478295f9aa8ab1b83908",
      "b6cc22bec624493cb11da8a6369b11a8",
      "8d682b2a66a14e1ab3345c27b969de1e",
      "df8df8420bc342369e233a8a16385efc",
      "e236994a32f244b88030884c0495a72e",
      "3aa278af5eb54745a56e4b61af3beac2",
      "fa14c3851c6447c0a28ea94314a832ad",
      "29edf5a4a8884ce5a18f71def1ae921b",
      "505a0b8197ab4680a3c51aff901b1569",
      "94e07dc24c01434aa319100832e9d21c",
      "8ff989a5005542a3a737c38f7959f2ce",
      "37c4db5b41c94c99b58627004c25e12d",
      "54eef9cd5ba54636ba508be7c5e65d70",
      "2a83a645921d4970b8ed0b4b29f123bc"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-02-19T13:31:49.656494Z",
     "iopub.status.busy": "2025-02-19T13:31:49.656160Z"
    },
    "id": "qb0sCnSFsHJl",
    "outputId": "d60f00f4-16a8-448f-f3bb-4bc152a7165f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:31:01.437663: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-19 13:31:01.438385: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-19 13:31:01.443328: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-19 13:31:01.453036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739989861.467368    1027 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739989861.470655    1027 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 13:31:01.487177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739989863.102450    1027 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-02-19 13:31:03.209449: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-02-19 13:31:05.215780: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-19 13:31:07.285081: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "Tokenizer vocabulary size: 134\n",
      "\n",
      "Training for parameter combination:  1, \n",
      "\n",
      "trial 1 for current parameter setting...\n",
      "Epoch 01 | Training Loss: 0.6807 | Val Loss: 0.6712 | Accuracy: 0.5888 | Precision: 0.5557 | Recall: 0.7578\n",
      "Epoch 02 | Training Loss: 0.6637 | Val Loss: 0.6644 | Accuracy: 0.6012 | Precision: 0.5719 | Recall: 0.7054\n",
      "Epoch 03 | Training Loss: 0.6604 | Val Loss: 0.6626 | Accuracy: 0.6104 | Precision: 0.5896 | Recall: 0.6460\n",
      "Epoch 04 | Training Loss: 0.6586 | Val Loss: 0.6626 | Accuracy: 0.6098 | Precision: 0.5827 | Recall: 0.6877\n",
      "Epoch 05 | Training Loss: 0.6566 | Val Loss: 0.6632 | Accuracy: 0.6070 | Precision: 0.5769 | Recall: 0.7100\n",
      "Epoch 06 | Training Loss: 0.6551 | Val Loss: 0.6607 | Accuracy: 0.6092 | Precision: 0.5860 | Recall: 0.6605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 302\u001b[0m\n\u001b[1;32m    300\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(predictions, y_batch)\n\u001b[1;32m    301\u001b[0m     grads \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch)\n\u001b[0;32m--> 302\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_value\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m    305\u001b[0m epoch_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:383\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    382\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    445\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:511\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:120\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    118\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m    119\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_reduce_sum_gradients(grads_and_vars)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:134\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py:131\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/optimizers/adam.py:121\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    118\u001b[0m gradient \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(gradient, variable\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    119\u001b[0m local_step \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, variable\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    120\u001b[0m beta_1_power \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mpower(\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m, local_step\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m beta_2_power \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mpower(\n\u001b[1;32m    124\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2, variable\u001b[38;5;241m.\u001b[39mdtype), local_step\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    127\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_momentums[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_variable_index(variable)]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/ops/core.py:803\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Cast(dtype\u001b[38;5;241m=\u001b[39mdtype)(x)\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:201\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_traceback_filtering_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    141\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# In some very rare cases,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# accessible from inside this function\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:47\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_traceback_filtering_enabled\u001b[39m():\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    was called).\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ENABLE_TRACEBACK_FILTERING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "tokenization_table = {'model':[], 'method': [], 'vocab_size':[], 'sequence_len':[]}\n",
    "\n",
    "metric_log = {'model':[], 'seed': [], 'layers':[], 'activation': [], 'optimizer': [], 'learning_rate': [], 'test_accuracy': [], 'test_loss': [], 'precision': [], 'recall': []}\n",
    "\n",
    "visited = []\n",
    "\n",
    "def set_new_seed(trial_number):\n",
    "    \"\"\"set new seed for every trial\n",
    "\n",
    "    Args:\n",
    "        trial_number (int): trial number\n",
    "\n",
    "    Returns:\n",
    "        [None]: None\n",
    "    \"\"\"\n",
    "    random_seed = random.randint(0, 2**32-1)\n",
    "    tf.random.set_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    return random_seed\n",
    "\n",
    "# -------------------------------\n",
    "# Original MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, layers, activation, device=None):\n",
    "        \"\"\"\n",
    "        layers: list containing dimensions of all layers\n",
    "        activation: str, indicating choice of activation function\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "        # list of all weights and biases\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        # Initialize weights and biases for each layer\n",
    "        for l in range(len(self.layers)):\n",
    "            if l == 0:\n",
    "                continue\n",
    "            W = tf.Variable(tf.random.normal([\n",
    "                            self.layers[l-1], self.layers[l]], \n",
    "                            stddev=0.1))\n",
    "            self.W.append(W)\n",
    "            b = tf.Variable(tf.zeros([1, self.layers[l]]))\n",
    "            self.b.append(b)\n",
    "        \n",
    "        # List of variables to update during backpropagation\n",
    "        self.variables = self.W + self.b\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            # use '/GPU:0' instead of 'gpu:0' for using gpu on mac\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        return grads\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        z = tf.cast(X, dtype=tf.float32)\n",
    "        for l in range(len(self.W)-1):\n",
    "            h = tf.matmul(z, self.W[l]) + self.b[l]\n",
    "            # activation function\n",
    "            if activation == 'ReLU':\n",
    "                z = tf.nn.relu(h)\n",
    "            elif activation == 'Tanh':\n",
    "                z = tf.nn.tanh(h)\n",
    "            elif activation == 'LeakyReLU':\n",
    "                z = tf.nn.leaky_relu(h)\n",
    "        \n",
    "        # Output layer (logits)\n",
    "        output = tf.matmul(z, self.W[-1]) + self.b[-1]\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def char_level_tokenizer(texts, char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "tokenizer = char_level_tokenizer(train_texts, char_level=True)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train = texts_to_bow(tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(tokenizer, test_texts)\n",
    "\n",
    "# record tokenization details\n",
    "tokenization_table['model'] = 'mlp'\n",
    "tokenization_table['method'] = 'character'\n",
    "tokenization_table['vocab_size'] = len(tokenizer.word_index) + 1\n",
    "tokenization_table['sequence_len'] = len(X_train[0])\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# List of dimensions in all the layers of the network\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "hidden_layers = [1, 2, 3]\n",
    "hidden_sizes = [128, 256, 512]\n",
    "output_size = 2\n",
    "\n",
    "# activation\n",
    "activations = ['ReLU', 'Tanh', 'LeakyReLU']\n",
    "\n",
    "# learning rate\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# batch size\n",
    "batch_sizes = [32, 64, 128]\n",
    "# optimizer\n",
    "optims = ['SGD', 'Adam', 'RMSProp']\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "def generate_params():\n",
    "    while True:\n",
    "        n_hidden = random.choice(hidden_layers)\n",
    "        activation = random.choice(activations)\n",
    "        lr = random.choice(learning_rates)\n",
    "        optim = random.choice(optims)\n",
    "        batch_size = random.choice(batch_sizes)\n",
    "        parameters = [n_hidden, activation, lr, optim, batch_size]\n",
    "        if not parameters in visited:\n",
    "            visited.append(parameters)\n",
    "            return parameters\n",
    "\n",
    "trials = 60\n",
    "\n",
    "while trials >=0:\n",
    "    print(f\"\\nTraining for parameter combination:  {60-trials+1}, \\n\")\n",
    "    random_seed = set_new_seed(trials)\n",
    "\n",
    "    parameters = generate_params()\n",
    "    nn_layers = [X_train.shape[1]] + hidden_sizes[:parameters[0]] + [2]\n",
    "    activation = parameters[1]\n",
    "    lr = parameters[2]\n",
    "    optim = parameters[3]\n",
    "    batch_size = parameters[-1]\n",
    "    \n",
    "    best_test_acc = -100\n",
    "    best_precision = None\n",
    "    best_recall = None\n",
    "    best_loss = None\n",
    "    for i in range(3):\n",
    "        print(f\"trial {i+1} for current parameter setting...\")\n",
    "        # Initialize the optimizer\n",
    "        if optim == 'Adam':\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "        elif optim == 'SGD':\n",
    "            optimizer = tf.optimizers.SGD(learning_rate=lr)\n",
    "        elif optim == 'RMSProp':\n",
    "            optimizer = tf.optimizers.RMSprop(learning_rate=lr)\n",
    "            \n",
    "        # Instantiate the MLP model.\n",
    "        model = MLP(nn_layers, activation, device='gpu')\n",
    "\n",
    "        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data at the start of each epoch.\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_batches):\n",
    "                start = i * batch_size\n",
    "                end = min((i+1) * batch_size, X_train.shape[0])\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                # Compute gradients and update weights.\n",
    "                # with tf.GradientTape() as tape:\n",
    "                #     predictions = model.forward(X_batch)\n",
    "                #     loss_value = model.loss(predictions, y_batch)\n",
    "                # grads = tape.gradient(loss_value, model.variables)\n",
    "                predictions = model.forward(X_batch)\n",
    "                loss_value = model.loss(predictions, y_batch)\n",
    "                grads = model.backward(X_batch, y_batch)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "                epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "            epoch_loss /= X_train.shape[0]\n",
    "\n",
    "            # Evaluate on validation set.\n",
    "            val_logits = model.forward(X_val)\n",
    "            val_loss = model.loss(val_logits, y_val).numpy()\n",
    "            val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "            true_val = np.argmax(y_val, axis=1)\n",
    "            accuracy = np.mean(val_preds == true_val)\n",
    "            precision = precision_score(true_val, val_preds)\n",
    "            recall = recall_score(true_val, val_preds)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "\n",
    "        # -------------------------------\n",
    "        # Final Evaluation on Test Set\n",
    "        # -------------------------------\n",
    "        num_batches = int(np.ceil(X_test.shape[0] / batch_size))\n",
    "        test_precision = 0.0\n",
    "        test_recall = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        true_test = []\n",
    "        test_preds = []\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        for b in range(num_batches):\n",
    "            start = b*batch_size\n",
    "            end = min((b+1)*batch_size, X_test.shape[0])\n",
    "            X_batch = X_test[start: end]\n",
    "            test_output = tf.nn.softmax(model.forward(X_batch), axis=1)\n",
    "            y_batch = y_test[start: end]\n",
    "            test_loss = model.loss(test_output, y_batch).numpy()\n",
    "            predictions = np.argmax(test_output.numpy(), axis=1)\n",
    "            test_preds.extend(predictions)\n",
    "            labels = np.argmax(y_batch, axis=1)\n",
    "            true_test.extend(labels)\n",
    "\n",
    "        test_accuracy = np.mean(np.array(test_preds) == np.array(true_test))\n",
    "        test_precision = precision_score(true_test, test_preds)\n",
    "        test_recall = recall_score(true_test, test_preds)\n",
    "        if test_accuracy > best_test_acc:\n",
    "            best_test_acc = test_accuracy\n",
    "            best_precision = test_precision\n",
    "            best_recall = test_recall\n",
    "            best_loss = test_loss\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "            f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    metric_log['model'].append('mlp_char_tokens')\n",
    "    metric_log['seed'].append(random_seed)\n",
    "    metric_log['layers'].append(parameters[0])\n",
    "    metric_log['activation'].append(activation)\n",
    "    metric_log['optimizer'].append(optim)\n",
    "    metric_log['learning_rate'].append(lr)\n",
    "    metric_log['test_accuracy'].append(round(best_test_acc, 4))\n",
    "    metric_log['test_loss'].append(round(best_loss, 4))\n",
    "    metric_log['precision'].append(round(best_precision, 4))\n",
    "    metric_log['recall'].append(round(best_recall, 4))\n",
    "    trials -= 1\n",
    "\n",
    "\n",
    "tokenization_df = pd.DataFrame(tokenization_table)\n",
    "tokenization_df.to_csv('char_tokenization_details.csv', index=False)\n",
    "\n",
    "print(\"tokenization details:\")\n",
    "print(tokenization_df.head())\n",
    "\n",
    "exp_df = pd.DataFrame(metric_log)\n",
    "exp_df.to_csv('char_token_experiments_log.csv', index=False)\n",
    "print(\"experiments details:\")\n",
    "print(exp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word level tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 12:27:26.615966: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-17 12:27:28.415958: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "Tokenizer vocabulary size: 80169\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4148 | Val Loss: 0.2811 | Accuracy: 0.8882 | Precision: 0.8749 | Recall: 0.8977\n",
      "Epoch 02 | Training Loss: 0.1165 | Val Loss: 0.3039 | Accuracy: 0.8818 | Precision: 0.8526 | Recall: 0.9142\n",
      "Epoch 03 | Training Loss: 0.0224 | Val Loss: 0.3989 | Accuracy: 0.8846 | Precision: 0.8905 | Recall: 0.8688\n",
      "Epoch 04 | Training Loss: 0.0034 | Val Loss: 0.4614 | Accuracy: 0.8846 | Precision: 0.8722 | Recall: 0.8927\n",
      "Epoch 05 | Training Loss: 0.0008 | Val Loss: 0.5076 | Accuracy: 0.8838 | Precision: 0.8723 | Recall: 0.8907\n",
      "Epoch 06 | Training Loss: 0.0004 | Val Loss: 0.5385 | Accuracy: 0.8832 | Precision: 0.8716 | Recall: 0.8903\n",
      "Epoch 07 | Training Loss: 0.0003 | Val Loss: 0.5631 | Accuracy: 0.8828 | Precision: 0.8724 | Recall: 0.8882\n",
      "Epoch 08 | Training Loss: 0.0002 | Val Loss: 0.5856 | Accuracy: 0.8832 | Precision: 0.8698 | Recall: 0.8927\n",
      "Epoch 09 | Training Loss: 0.0001 | Val Loss: 0.6043 | Accuracy: 0.8830 | Precision: 0.8688 | Recall: 0.8936\n",
      "Epoch 10 | Training Loss: 0.0001 | Val Loss: 0.6193 | Accuracy: 0.8830 | Precision: 0.8718 | Recall: 0.8894\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 261\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Final Evaluation on Test Set\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m test_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(test_logits, y_test)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    263\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(test_logits\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_output(X)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\n",
      "Cell \u001b[0;32mIn[10], line 86\u001b[0m, in \u001b[0;36mMLP.compute_output\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mCustom method to compute the output tensor during the forward pass.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Cast X to float32\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m X_tf \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# First hidden layer\u001b[39;00m\n\u001b[1;32m     88\u001b[0m h1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(X_tf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\n",
      "File \u001b[0;32m~/miniforge3/envs/tfmetal/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tfmetal/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tokenization_table = {'model':[], 'method': [], 'vocab_size':[], 'sequence_len':[]}\n",
    "\n",
    "metric_log = {'model':[], 'seed': [], 'layers':[], 'activation': [], 'optimizer': [], 'learning_rate': [], 'test_accuracy': [], 'test_loss': [], 'precision': [], 'recall': []}\n",
    "\n",
    "visited = []\n",
    "\n",
    "def set_new_seed(trial_number):\n",
    "    \"\"\"set new seed for every trial\n",
    "\n",
    "    Args:\n",
    "        trial_number (int): trial number\n",
    "\n",
    "    Returns:\n",
    "        [None]: None\n",
    "    \"\"\"\n",
    "    random_seed = random.randint(0, 2**32-1)\n",
    "    tf.random.set_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    return random_seed\n",
    "\n",
    "# -------------------------------\n",
    "# Original MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, layers, activation, device=None):\n",
    "        \"\"\"\n",
    "        layers: list containing dimensions of all layers\n",
    "        activation: str, indicating choice of activation function\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "        # list of all weights and biases\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for l in range(len(self.layers)):\n",
    "            if l == 0:\n",
    "                continue\n",
    "            W = tf.Variable(tf.random.normal([\n",
    "                            self.layers[l-1], self.layers[l]], \n",
    "                            stddev=0.1))\n",
    "            self.W.append(W)\n",
    "            b = tf.Variable(tf.zeros([1, self.layers[l]]))\n",
    "            self.b.append(b)\n",
    "\n",
    "        # List of variables to update during backpropagation\n",
    "        self.variables = self.W + self.b\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            # use '/GPU:0' instead of 'gpu:0' for using gpu on mac\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        return grads\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        z = tf.cast(X, dtype=tf.float32)\n",
    "        for l in range(len(self.layers)-1):\n",
    "            h = tf.matmul(z, self.W[l]) + self.b[l]\n",
    "            # activation function\n",
    "            if activation == 'ReLU':\n",
    "                self.activation = tf.nn.relu(h)\n",
    "            elif activation == 'Tanh':\n",
    "                self.activation = tf.nn.tanh(h)\n",
    "            elif activation == 'LeakyReLU':\n",
    "                self.activation = tf.nn.leaky_relu(h)\n",
    "        \n",
    "        # Output layer (logits)\n",
    "        output = tf.matmul(z, self.W[-1]) + self.b[-1]\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def char_level_tokenizer(texts, char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the word-level tokenizer on the training texts.\n",
    "tokenizer = char_level_tokenizer(train_texts, char_level=False)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Convert texts to bag-of-words representation.\n",
    "X_train = texts_to_bow(tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(tokenizer, test_texts)\n",
    "\n",
    "# record tokenization details\n",
    "tokenization_table['model'] = 'mlp'\n",
    "tokenization_table['method'] = 'word'\n",
    "tokenization_table['vocab_size'] = len(tokenizer.word_index) + 1\n",
    "tokenization_table['sequence_len'] = X_train.shape[1]\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# List of dimensions in all the layers of the network\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "hidden_layers = [1, 2, 3]\n",
    "hidden_sizes = [128, 256, 512]\n",
    "output_size = 2\n",
    "\n",
    "# activation\n",
    "activations = ['ReLU', 'Tanh', 'LeakyReLU']\n",
    "\n",
    "# learning rate\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "# batch size\n",
    "batch_sizes = [32, 64, 128]\n",
    "# optimizer\n",
    "optims = ['SGD', 'Adam', 'RMSProp']\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "epochs = 10\n",
    "\n",
    "def generate_params():\n",
    "    while True:\n",
    "        n_hidden = random.choice(hidden_layers)\n",
    "        activation = random.choice(activations)\n",
    "        lr = random.choice(learning_rates)\n",
    "        optim = random.choice(optims)\n",
    "        batch_size = random.choice(batch_sizes)\n",
    "        parameters = [n_hidden, activation, lr, optim, batch_size]\n",
    "        if not parameters in visited:\n",
    "            visited.append(parameters)\n",
    "            return parameters\n",
    "\n",
    "trials = 60\n",
    "\n",
    "while trials >=0:\n",
    "    print(f\"\\nTraining for parameter combination: {60-trials+1}, \\n\")\n",
    "    random_seed = set_new_seed(trials)\n",
    "\n",
    "    parameters = generate_params()\n",
    "    nn_layers = [X_train.shape[1]] + hidden_sizes[:parameters[0]] + [2]\n",
    "    activation = parameters[1]\n",
    "    lr = parameters[2]\n",
    "    optim = parameters[3]\n",
    "    batch_size = parameters[-1]\n",
    "\n",
    "    best_test_acc = -100\n",
    "    best_precision = None\n",
    "    best_recall = None\n",
    "    best_loss = None\n",
    "    for i in range(3):\n",
    "        print(f\"trial {i+1} for current parameter setting...\")\n",
    "        # Initialize the optimizer\n",
    "        if optim == 'Adam':\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=lr)\n",
    "        elif optim == 'SGD':\n",
    "            optimizer = tf.optimizers.SGD(learning_rate=lr)\n",
    "        elif optim == 'RMSProp':\n",
    "            optimizer = tf.optimizers.RMSprop(learning_rate=lr)\n",
    "            \n",
    "        # Instantiate the MLP model.\n",
    "        model = MLP(nn_layers, activation, device='gpu')\n",
    "\n",
    "        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "        print(f\"\\nTraining for trial {60-trials+1}...\\n\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data at the start of each epoch.\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_batches):\n",
    "                start = i * batch_size\n",
    "                end = min((i+1) * batch_size, X_train.shape[0])\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                # Compute gradients and update weights.\n",
    "                # with tf.GradientTape() as tape:\n",
    "                #     predictions = model.forward(X_batch)\n",
    "                #     loss_value = model.loss(predictions, y_batch)\n",
    "                # grads = tape.gradient(loss_value, model.variables)\n",
    "                predictions = model.forward(X_batch)\n",
    "                loss_value = model.loss(predictions, y_batch)\n",
    "                grads = model.backward(X_batch, y_batch)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "                epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "            epoch_loss /= X_train.shape[0]\n",
    "\n",
    "            # Evaluate on validation set.\n",
    "            val_logits = model.forward(X_val)\n",
    "            val_loss = model.loss(val_logits, y_val).numpy()\n",
    "            val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "            true_val = np.argmax(y_val, axis=1)\n",
    "            accuracy = np.mean(val_preds == true_val)\n",
    "            precision = precision_score(true_val, val_preds)\n",
    "            recall = recall_score(true_val, val_preds)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # Final Evaluation on Test Set\n",
    "        # -------------------------------\n",
    "        num_batches = int(np.ceil(X_test.shape[0] / batch_size))\n",
    "        test_precision = 0.0\n",
    "        test_recall = 0.0\n",
    "        test_accuracy = 0.0\n",
    "        true_test = []\n",
    "        test_preds = []\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        for b in range(num_batches):\n",
    "            start = b*batch_size\n",
    "            end = min((b+1)*batch_size, X_test.shape[0])\n",
    "            X_batch = X_test[start: end]\n",
    "            test_logits = tf.nn.softmax(model.forward(X_batch), axis=1)\n",
    "            y_batch = y_test[start: end]\n",
    "            test_loss = model.loss(test_logits, y_batch).numpy()\n",
    "            predictions = np.argmax(test_logits.numpy(), axis=1)\n",
    "            test_preds.extend(predictions)\n",
    "            labels = np.argmax(y_batch, axis=1)\n",
    "            true_test.extend(labels)\n",
    "\n",
    "        test_accuracy = np.mean(np.array(test_preds) == np.array(true_test))\n",
    "        test_precision = precision_score(true_test, test_preds)\n",
    "        test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "        if test_accuracy > best_test_acc:\n",
    "            best_test_acc = test_accuracy\n",
    "            best_precision = test_precision\n",
    "            best_recall = test_recall\n",
    "            best_loss = test_loss\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "            f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    metric_log['model'].append('mlp_word_tokens')\n",
    "    metric_log['seed'].append(random_seed)\n",
    "    metric_log['layers'].append(parameters[0])\n",
    "    metric_log['activation'].append(activation)\n",
    "    metric_log['optimizer'].append(optim)\n",
    "    metric_log['learning_rate'].append(lr)\n",
    "    metric_log['test_accuracy'].append(round(best_test_acc, 4))\n",
    "    metric_log['test_loss'].append(round(best_loss, 4))\n",
    "    metric_log['precision'].append(round(best_precision, 4))\n",
    "    metric_log['recall'].append(round(best_recall, 4))\n",
    "\n",
    "\n",
    "tokenization_df = pd.DataFrame(tokenization_table)\n",
    "tokenization_df.to_csv('word_tokenization_details.csv', index=False)\n",
    "\n",
    "print(\"tokenization details:\")\n",
    "print(tokenization_df.head())\n",
    "\n",
    "exp_df = pd.DataFrame(metric_log)\n",
    "exp_df.to_csv('word_token_experiments_log.csv', index=False)\n",
    "print(\"experiments details:\")\n",
    "print(exp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpy6iEakUWAZ"
   },
   "source": [
    "## Random MLP on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0MseUvkuqd0",
    "outputId": "743bfee2-02cb-4c85-8b53-a286f50d5989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 12:26:49.507020: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 12:26:51.397463: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "char_level_tokenizer() missing 1 required positional argument: 'char_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 170\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Preprocessing: Tokenization and Vectorization\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Build the character-level tokenizer on the training texts.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mchar_level_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer vocabulary size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Convert texts to bag-of-characters representation.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: char_level_tokenizer() missing 1 required positional argument: 'char_level'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "# -------------------------------\n",
    "# Original MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP_rnd(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden1 = size_hidden1\n",
    "        self.size_hidden2 = size_hidden2\n",
    "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
    "        self.size_output = size_output\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases for first hidden layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights and biases for second hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # List of variables to update during backpropagation\n",
    "        #self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        self.variables = [self.W3, self.b3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        return grads\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # First hidden layer\n",
    "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        z1 = tf.nn.relu(h1)\n",
    "        # Second hidden layer\n",
    "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
    "        z2 = tf.nn.relu(h2)\n",
    "        # Output layer (logits)\n",
    "        output = tf.matmul(z2, self.W3) + self.b3\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def char_level_tokenizer(texts, char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "tokenizer = char_level_tokenizer(train_texts, True)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train = texts_to_bow(tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(tokenizer, test_texts)\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train.shape[1]\n",
    "# Set hidden layer sizes as desired.\n",
    "size_hidden1 = 128\n",
    "size_hidden2 = 64\n",
    "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
    "size_output  = 2\n",
    "\n",
    "# Instantiate the MLP model.\n",
    "model = MLP_rnd(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data at the start of each epoch.\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i+1) * batch_size, X_train.shape[0])\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "\n",
    "        # Compute gradients and update weights.\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     predictions = model.forward(X_batch)\n",
    "        #     loss_value = model.loss(predictions, y_batch)\n",
    "        # grads = tape.gradient(loss_value, model.variables)\n",
    "        predictions = model.forward(X_batch)\n",
    "        loss_value = model.loss(predictions, y_batch)\n",
    "        grads = model.backward(X_batch, y_batch)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "    epoch_loss /= X_train.shape[0]\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    val_logits = model.forward(X_val)\n",
    "    val_loss = model.loss(val_logits, y_val).numpy()\n",
    "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "    true_val = np.argmax(y_val, axis=1)\n",
    "    accuracy = np.mean(val_preds == true_val)\n",
    "    precision = precision_score(true_val, val_preds)\n",
    "    recall = recall_score(true_val, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Final Evaluation on Test Set\n",
    "# -------------------------------\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_logits = model.forward(X_test)\n",
    "test_loss = model.loss(test_logits, y_test).numpy()\n",
    "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "true_test = np.argmax(y_test, axis=1)\n",
    "test_accuracy = np.mean(test_preds == true_test)\n",
    "test_precision = precision_score(true_test, test_preds)\n",
    "test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-wETEHOUGuB"
   },
   "source": [
    "## MLP with feedback alignment on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mG5AfD2DTdNy",
    "outputId": "fbed505a-3d2e-4227-89ed-a486b8b4a942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 02:09:42.889939: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-16 02:09:44.670319: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "Tokenizer vocabulary size: 134\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6810 | Val Loss: 0.6650 | Accuracy: 0.6048 | Precision: 0.5828 | Recall: 0.6502\n",
      "Epoch 02 | Training Loss: 0.6634 | Val Loss: 0.6645 | Accuracy: 0.6070 | Precision: 0.5810 | Recall: 0.6790\n",
      "Epoch 03 | Training Loss: 0.6629 | Val Loss: 0.6624 | Accuracy: 0.6126 | Precision: 0.5888 | Recall: 0.6663\n",
      "Epoch 04 | Training Loss: 0.6614 | Val Loss: 0.6630 | Accuracy: 0.6066 | Precision: 0.5803 | Recall: 0.6815\n",
      "Epoch 05 | Training Loss: 0.6629 | Val Loss: 0.6629 | Accuracy: 0.6060 | Precision: 0.6076 | Recall: 0.5289\n",
      "Epoch 06 | Training Loss: 0.6596 | Val Loss: 0.6615 | Accuracy: 0.6062 | Precision: 0.5826 | Recall: 0.6621\n",
      "Epoch 07 | Training Loss: 0.6585 | Val Loss: 0.6602 | Accuracy: 0.6108 | Precision: 0.5915 | Recall: 0.6374\n",
      "Epoch 08 | Training Loss: 0.6562 | Val Loss: 0.6619 | Accuracy: 0.6032 | Precision: 0.5750 | Recall: 0.6955\n",
      "Epoch 09 | Training Loss: 0.6553 | Val Loss: 0.6613 | Accuracy: 0.6086 | Precision: 0.5997 | Recall: 0.5792\n",
      "Epoch 10 | Training Loss: 0.6535 | Val Loss: 0.6628 | Accuracy: 0.6050 | Precision: 0.5759 | Recall: 0.7026\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6613 | Test Accuracy: 0.6071 | Test Precision: 0.5889 | Test Recall: 0.7096\n"
     ]
    }
   ],
   "source": [
    "class MLP_FA(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (Note: Not used in compute_output in this example)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden1 = size_hidden1\n",
    "        self.size_hidden2 = size_hidden2\n",
    "        self.size_hidden3 = size_hidden3  # (Currently not used)\n",
    "        self.size_output = size_output\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases for first hidden layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights and biases for second hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # Create fixed random feedback matrices for feedback alignment:\n",
    "        # B3: used to propagate the error from the output layer to the second hidden layer.\n",
    "        # It replaces the use of W3^T. Its shape is (size_output, size_hidden2).\n",
    "        self.B3 = tf.Variable(tf.random.normal([self.size_output, self.size_hidden2]), trainable=False)\n",
    "\n",
    "        # B2: used to propagate the error from the second hidden layer to the first hidden layer.\n",
    "        # Its shape is (size_hidden2, size_hidden1).\n",
    "        self.B2 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden1]), trainable=False)\n",
    "\n",
    "        # Define variables to be updated during training\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass using feedback alignment.\n",
    "        Computes gradients manually using fixed random feedback matrices.\n",
    "        X_train: Input data (numpy array)\n",
    "        y_train: One-hot encoded labels (numpy array)\n",
    "        Returns: List of gradients corresponding to [dW1, dW2, dW3, db1, db2, db3]\n",
    "        \"\"\"\n",
    "        # Cast input to float32 tensor\n",
    "        X_tf = tf.cast(X_train, tf.float32)\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        # First hidden layer\n",
    "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        a1 = tf.nn.relu(h1)\n",
    "        # Second hidden layer\n",
    "        h2 = tf.matmul(a1, self.W2) + self.b2\n",
    "        a2 = tf.nn.relu(h2)\n",
    "        # Output layer (logits)\n",
    "        logits = tf.matmul(a2, self.W3) + self.b3\n",
    "        # Softmax predictions\n",
    "        y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "        # --- Compute Output Error ---\n",
    "        # For cross-entropy with softmax, the derivative is (y_pred - y_true)\n",
    "        delta3 = y_pred - tf.cast(y_train, tf.float32)  # shape: (batch, size_output)\n",
    "        batch_size = tf.cast(tf.shape(X_tf)[0], tf.float32)\n",
    "\n",
    "        # --- Gradients for Output Layer ---\n",
    "        dW3 = tf.matmul(tf.transpose(a2), delta3) / batch_size\n",
    "        db3 = tf.reduce_mean(delta3, axis=0, keepdims=True)\n",
    "\n",
    "        # --- Feedback Alignment for Second Hidden Layer ---\n",
    "        # Instead of delta2 = (delta3 dot W3^T) * ReLU'(h2), use a fixed random matrix B3.\n",
    "        relu_grad_h2 = tf.cast(h2 > 0, tf.float32)\n",
    "        # delta3 has shape (batch, size_output) and B3 has shape (size_output, size_hidden2)\n",
    "        delta2 = tf.matmul(delta3, self.B3) * relu_grad_h2  # shape: (batch, size_hidden2)\n",
    "\n",
    "        dW2 = tf.matmul(tf.transpose(a1), delta2) / batch_size\n",
    "        db2 = tf.reduce_mean(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        # --- Feedback Alignment for First Hidden Layer ---\n",
    "        # Instead of delta1 = (delta2 dot W2^T) * ReLU'(h1), use a fixed random matrix B2.\n",
    "        relu_grad_h1 = tf.cast(h1 > 0, tf.float32)\n",
    "        # delta2 has shape (batch, size_hidden2) and B2 has shape (size_hidden2, size_hidden1)\n",
    "        delta1 = tf.matmul(delta2, self.B2) * relu_grad_h1  # shape: (batch, size_hidden1)\n",
    "\n",
    "        dW1 = tf.matmul(tf.transpose(X_tf), delta1) / batch_size\n",
    "        db1 = tf.reduce_mean(delta1, axis=0, keepdims=True)\n",
    "\n",
    "        return [dW1, dW2, dW3, db1, db2, db3]\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        z1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
    "        z2 = tf.nn.relu(h2)\n",
    "        output = tf.matmul(z2, self.W3) + self.b3\n",
    "        return output\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def char_level_tokenizer(texts, char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "tokenizer = char_level_tokenizer(train_texts, True)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train = texts_to_bow(tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(tokenizer, test_texts)\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train.shape[1]\n",
    "# Set hidden layer sizes as desired.\n",
    "size_hidden1 = 128\n",
    "size_hidden2 = 64\n",
    "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
    "size_output  = 2\n",
    "\n",
    "# Instantiate the MLP model.\n",
    "model = MLP_FA(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data at the start of each epoch.\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i+1) * batch_size, X_train.shape[0])\n",
    "        X_batch = X_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "\n",
    "        # Compute gradients and update weights.\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     predictions = model.forward(X_batch)\n",
    "        #     loss_value = model.loss(predictions, y_batch)\n",
    "        # grads = tape.gradient(loss_value, model.variables)\n",
    "        predictions = model.forward(X_batch)\n",
    "        loss_value = model.loss(predictions, y_batch)\n",
    "        grads = model.backward(X_batch, y_batch)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "    epoch_loss /= X_train.shape[0]\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    val_logits = model.forward(X_val)\n",
    "    val_loss = model.loss(val_logits, y_val).numpy()\n",
    "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "    true_val = np.argmax(y_val, axis=1)\n",
    "    accuracy = np.mean(val_preds == true_val)\n",
    "    precision = precision_score(true_val, val_preds)\n",
    "    recall = recall_score(true_val, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Final Evaluation on Test Set\n",
    "# -------------------------------\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_logits = model.forward(X_test)\n",
    "test_loss = model.loss(test_logits, y_test).numpy()\n",
    "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "true_test = np.argmax(y_test, axis=1)\n",
    "test_accuracy = np.mean(test_preds == true_test)\n",
    "test_precision = precision_score(true_test, test_preds)\n",
    "test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6j6mv098aASE"
   },
   "source": [
    "# Assignment 2 Todos\n",
    "\n",
    "## Overview\n",
    "- **Objective:**  \n",
    "  Modify your model’s text preprocessing by changing from character-level tokenization to word-level tokenization. Compare the performance of both tokenization methods. Additionally, perform hyper-parameter optimization by experimenting with various settings (learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions) and report your findings.\n",
    "\n",
    "## 1. Initial Setup\n",
    "- [x] **Set Random Seeds:**  \n",
    "  Ensure reproducibility by setting seeds for all random number generators (e.g., Python’s `random`, NumPy, TensorFlow/PyTorch).\n",
    "  \n",
    "- [x] **Prepare the Environment:**  \n",
    "  - Create a new or update an existing Jupyter Notebook.\n",
    "  - Ensure that all necessary libraries (e.g., NumPy, pandas, TensorFlow/PyTorch, matplotlib, etc.) are installed.\n",
    "  \n",
    "- [x] **Version Control:**  \n",
    "  Initialize a Git repository (if not already done) and commit your initial setup.\n",
    "\n",
    "## 2. Data Preprocessing\n",
    "- [x] **Load Dataset:**  \n",
    "  Load your dataset into the notebook.\n",
    "  \n",
    "- [x] **Tokenization:**\n",
    "  - **Character-Level Tokenization:**  \n",
    "    - Tokenize the text data at the character level.\n",
    "    - Save and log the processed data.\n",
    "  - **Word-Level Tokenization:**  \n",
    "    - Modify the tokenization process to tokenize the text by words.\n",
    "    - Save and log the processed data.\n",
    "    \n",
    "- [x] **Comparison:**  \n",
    "  - Create a section in your notebook to compare the two tokenization approaches.\n",
    "  - Visualize or tabulate differences in vocabulary size, sequence lengths, and other relevant metrics.\n",
    "\n",
    "## 3. Model Architecture\n",
    "- [x] **Define the Model:**  \n",
    "  Develop a model (or models) that can handle both tokenization types. Include the following adjustable hyper-parameters:\n",
    "  - Learning rate\n",
    "  - Number of hidden layers\n",
    "  - Hidden sizes (neurons per layer)\n",
    "  - Batch sizes\n",
    "  - Optimizers (e.g., Adam, SGD, RMSProp)\n",
    "  - Activation functions (e.g., ReLU, Tanh, LeakyReLU)\n",
    "\n",
    "## 4. Hyper-Parameter Optimization\n",
    "- [ ] **Experiment Setup:**  \n",
    "  For each hyper-parameter configuration, perform at least 3 different tests to ensure robustness.\n",
    "  \n",
    "- [x] **Grid/Random Search:**  \n",
    "  Set up a search over the following hyper-parameter ranges (example values provided):\n",
    "  - **Learning Rate:** `[0.001, 0.0005, 0.0001]`\n",
    "  - **Hidden Layers:** `[1, 2, 3]`\n",
    "  - **Hidden Sizes:** `[128, 256, 512]`\n",
    "  - **Batch Sizes:** `[32, 64, 128]`\n",
    "  - **Optimizers:** `[Adam, SGD, RMSProp]`\n",
    "  - **Activation Functions:** `[ReLU, Tanh, LeakyReLU]`\n",
    "  \n",
    "- [x] **Logging:**  \n",
    "  Record the results (accuracy, loss, etc.) for each configuration in tables or charts.\n",
    "\n",
    "## 5. Model Training and Evaluation\n",
    "- [ ] **Training with Each Configuration:**  \n",
    "  Run experiments for both tokenization approaches with each set of hyper-parameters:\n",
    "  - Train the model at least 3 times per configuration (keeping the seed constant at this stage).\n",
    "  - Log training and validation performance.\n",
    "  \n",
    "- [ ] **Identify the Best Model:**  \n",
    "  Select the best performing configuration based on validation metrics (e.g., accuracy).\n",
    "\n",
    "## 6. Final Experiments\n",
    "- [ ] **Robustness Check:**  \n",
    "  Once the best model is identified:\n",
    "  - Re-run the experiments at least 3 times with different random seeds.\n",
    "  - Record the performance (accuracy) for each run.\n",
    "  \n",
    "- [ ] **Statistical Reporting:**  \n",
    "  - Compute the **mean accuracy** and **standard error** across these runs.\n",
    "  - Include these statistics in your report.\n",
    "\n",
    "## 7. Documentation and Reporting\n",
    "- [x] **Jupyter Notebook:**  \n",
    "  - Ensure that your notebook is well-commented and clearly documents each step.\n",
    "  - Include code cells for setting seeds, data preprocessing, model building, training, evaluation, and visualization.\n",
    "  \n",
    "- [ ] **Detailed Report (Word Document):**  \n",
    "  Prepare a report that includes:\n",
    "  - **Introduction:** Objectives and overview of the work.\n",
    "  - **Methodology:** Detailed explanation of tokenization changes and hyper-parameter optimization strategy.\n",
    "  - **Experiments and Results:**  \n",
    "    - Comparison between character-level and word-level tokenization.\n",
    "    - Tables/graphs for hyper-parameter experiments.\n",
    "    - Final model performance with mean accuracy and standard error.\n",
    "  - **Discussion:** Analysis of results, challenges encountered, and insights.\n",
    "  - **Conclusion:** Summarize the key findings.\n",
    "  \n",
    "- [ ] **Submission:**  \n",
    "  - Submit your Jupyter Notebook.\n",
    "  - Submit your Word document report.\n",
    "  - Ensure that both files are included in your repository or submission package.\n",
    "\n",
    "## 8. Final Checklist\n",
    "- [ ] All experiments have at least 3 different tests.\n",
    "- [ ] Random seeds are set before any experiment.\n",
    "- [ ] Hyper-parameter optimization covers changes in learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions.\n",
    "- [ ] The best model’s performance is verified with experiments on different seeds.\n",
    "- [ ] Best model should be compared with random model shown above.\n",
    "- [ ] The report clearly documents the methodology, experiments, results, and final conclusions.\n",
    "- [ ] If experiments are shown with deeper MLP_FA with best settings (Extra credits -- 2 points)\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> Keep thorough logs and document any observations during your experiments. Clear documentation is key to reproducibility and understanding your results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e66320306b44c65bce10ade0c4d2eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ee8c01d75c541218b8bc09ff1f94cde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "12dc246bd54c4e3ea3747c65c2e17014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1547a0be80f84b32a254aea474d6867a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1853768c4e554ae0b1ade9b6e62d771b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a10d0b1fd154491a8888340c8ebfe38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20a84906121842f48543c1bad206d799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb85ef2782754246b66058f75317feed",
       "IPY_MODEL_5138ab3ff1dc4cc883a434e1a7caa14a",
       "IPY_MODEL_a7ea683bf59f4687bd3eadba6298a883"
      ],
      "layout": "IPY_MODEL_d074fc35f4864f55a1e4ebdca6e07530"
     }
    },
    "21b3131b402b4b14991f046d9a3a8be0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "250a431c05cb4878a428cf415f702b36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "25795eca2fe34d14845d14f3295a9ec2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "259e6840f71c4a29b64725a4d921965f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "26b2ec9c903547f886effcacfeea8629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e014cd8004684dd0bf2513e8dcb9e9f8",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff5c2c87b086478295f9aa8ab1b83908",
      "value": 50000
     }
    },
    "29edf5a4a8884ce5a18f71def1ae921b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "2a83a645921d4970b8ed0b4b29f123bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f96003dea1542f58a9ff716fe8713ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94a33644645a4c27a8b781908bb5c92b",
      "placeholder": "​",
      "style": "IPY_MODEL_c96784cc747c4dd6b95621289526af8a",
      "value": "Generating test examples...:  82%"
     }
    },
    "35d8ba6f21ab445e856039f4c75082b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37c4db5b41c94c99b58627004c25e12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3aa278af5eb54745a56e4b61af3beac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ff989a5005542a3a737c38f7959f2ce",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37c4db5b41c94c99b58627004c25e12d",
      "value": 50000
     }
    },
    "3bdad631a663468e947008ba9a952299": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb323561a3de4b8f85a43a075d00119e",
      "placeholder": "​",
      "style": "IPY_MODEL_d704ab225ada4134bf1ce4bb830a6354",
      "value": "Generating splits...: 100%"
     }
    },
    "3c9c79a8361a48ea9128f738e67941b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3db2d29d0c1649218f2f2c604993727a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dec77d473c84931836cfa6a0db79ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6ea7ea17fe443ed816de452ba64b2f1",
      "placeholder": "​",
      "style": "IPY_MODEL_5af6d5859b724a3ba77f579df40b71d4",
      "value": " 0/25000 [00:00&lt;?, ? examples/s]"
     }
    },
    "3ebcc18d220745338c4ec8de6127e48e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ec9320a7eb74e4c98b230fba6d8a2ce",
      "placeholder": "​",
      "style": "IPY_MODEL_21b3131b402b4b14991f046d9a3a8be0",
      "value": " 1/1 [00:16&lt;00:00, 16.30s/ url]"
     }
    },
    "40019e582a064de589847a3da1175eca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40845adffae1455689ad5a1559559fa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d486058c5d47a38ac3adb0cf2bc6d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40019e582a064de589847a3da1175eca",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7eb9b35910a64aecaa958983ea078dad",
      "value": 25000
     }
    },
    "43b6dbdb5b6a42d198d384ee7fa4dd29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "444ec309e9cc4828b3ae669d28901abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46836bd062e74b14a9e5ae1f1d48a184": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b056533e06a48a5a81d15cf44b961b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8e3a8be14b742209b8ef74200a2cd9f",
      "placeholder": "​",
      "style": "IPY_MODEL_1853768c4e554ae0b1ade9b6e62d771b",
      "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.MK1RO0_1.0.0/imdb_reviews-train.tfrecord*...:   0%"
     }
    },
    "505a0b8197ab4680a3c51aff901b1569": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5138ab3ff1dc4cc883a434e1a7caa14a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_250a431c05cb4878a428cf415f702b36",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8c785ed43d84e4e856476928b2a49e9",
      "value": 1
     }
    },
    "54eef9cd5ba54636ba508be7c5e65d70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5af6d5859b724a3ba77f579df40b71d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b130b325efa49f99be9b52994686a0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9b7a63ccc5f462abb99fa20caa65cc4",
      "placeholder": "​",
      "style": "IPY_MODEL_0e66320306b44c65bce10ade0c4d2eeb",
      "value": " 18669/25000 [00:05&lt;00:01, 4882.97 examples/s]"
     }
    },
    "5e72ea42a5714ba5bf783e99af1ffe8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b411ae301a9c4cac9336c8f1bb18c135",
       "IPY_MODEL_ca846e4a00d5439b89615b29b94dd268",
       "IPY_MODEL_be89b15c9d9e48dda2b0819689faa365"
      ],
      "layout": "IPY_MODEL_e0716ea457b54fa58b4eef100fab9335"
     }
    },
    "62010f0361854b66afd987c8bd7e15f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "677c67b52ca14e2d88114d167d065797": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "708aac86918f45aaa50940f7122b36ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77a48de94d2948b0ae340c92000cf7e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bdad631a663468e947008ba9a952299",
       "IPY_MODEL_9610d42f53d441c8940e894954e11339",
       "IPY_MODEL_b36275d5ce954d7ab7b442bb60729146"
      ],
      "layout": "IPY_MODEL_aad3c5ae005a4124acd9285769a215e1"
     }
    },
    "79cd58e72bd4406698ad3979a6c9ba6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25795eca2fe34d14845d14f3295a9ec2",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0a178cf8e0d4bbdb4b521a693ccadbe",
      "value": 25000
     }
    },
    "7df5f5059e5f49c9968599942999833b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "7eb9b35910a64aecaa958983ea078dad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ec9320a7eb74e4c98b230fba6d8a2ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "816a5390057d4567915bf34ad0b54c41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "825e86dc8b6046a0b60e728a911f99f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_708aac86918f45aaa50940f7122b36ab",
      "placeholder": "​",
      "style": "IPY_MODEL_46836bd062e74b14a9e5ae1f1d48a184",
      "value": "Generating unsupervised examples...:  95%"
     }
    },
    "85cfedddc7914481a99c93ae7f349851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88bf3f3840dd4be894756fc2474b84ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b056533e06a48a5a81d15cf44b961b7",
       "IPY_MODEL_a4903235f0f0406d9c0bee0678bf8805",
       "IPY_MODEL_3dec77d473c84931836cfa6a0db79ecb"
      ],
      "layout": "IPY_MODEL_62010f0361854b66afd987c8bd7e15f5"
     }
    },
    "8d682b2a66a14e1ab3345c27b969de1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f11f250de6448ad87a3363ffcb4e243": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "8ff989a5005542a3a737c38f7959f2ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94a33644645a4c27a8b781908bb5c92b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94e07dc24c01434aa319100832e9d21c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9610d42f53d441c8940e894954e11339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a10d0b1fd154491a8888340c8ebfe38",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85cfedddc7914481a99c93ae7f349851",
      "value": 3
     }
    },
    "98a5b8121fe341299996d32f52261fe6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc72e5be2694018a37b7861ccea3ade": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f96003dea1542f58a9ff716fe8713ee",
       "IPY_MODEL_79cd58e72bd4406698ad3979a6c9ba6d",
       "IPY_MODEL_ad10809f5e9045d9905cd083bb346f21"
      ],
      "layout": "IPY_MODEL_0ee8c01d75c541218b8bc09ff1f94cde"
     }
    },
    "a4903235f0f0406d9c0bee0678bf8805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43b6dbdb5b6a42d198d384ee7fa4dd29",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_259e6840f71c4a29b64725a4d921965f",
      "value": 25000
     }
    },
    "a614a6f07a7e42baaea992036e16612e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ea7ea17fe443ed816de452ba64b2f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ea683bf59f4687bd3eadba6298a883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_677c67b52ca14e2d88114d167d065797",
      "placeholder": "​",
      "style": "IPY_MODEL_3db2d29d0c1649218f2f2c604993727a",
      "value": " 80/80 [00:16&lt;00:00, 11.08 MiB/s]"
     }
    },
    "a85a9cfd1b254b7b98ce7a81661b96ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a8e3a8be14b742209b8ef74200a2cd9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aad3c5ae005a4124acd9285769a215e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "ad10809f5e9045d9905cd083bb346f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98a5b8121fe341299996d32f52261fe6",
      "placeholder": "​",
      "style": "IPY_MODEL_1547a0be80f84b32a254aea474d6867a",
      "value": " 20375/25000 [00:03&lt;00:00, 7305.65 examples/s]"
     }
    },
    "b36275d5ce954d7ab7b442bb60729146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40845adffae1455689ad5a1559559fa0",
      "placeholder": "​",
      "style": "IPY_MODEL_35d8ba6f21ab445e856039f4c75082b1",
      "value": " 3/3 [00:27&lt;00:00,  9.45s/ splits]"
     }
    },
    "b379d846511c4e17b6ca15c1f6662aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b411ae301a9c4cac9336c8f1bb18c135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c9c79a8361a48ea9128f738e67941b4",
      "placeholder": "​",
      "style": "IPY_MODEL_12dc246bd54c4e3ea3747c65c2e17014",
      "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.MK1RO0_1.0.0/imdb_reviews-test.tfrecord*...:   0%"
     }
    },
    "b594fbc34d5d40d4abae1759c81972af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6cc22bec624493cb11da8a6369b11a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71a01539a0f4b0c89bf87eba3780fcc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb323561a3de4b8f85a43a075d00119e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd5890b2ca084272990e5e16f6b81d8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3cbd7d3127f4cefb81665e7f7d7c004",
       "IPY_MODEL_40d486058c5d47a38ac3adb0cf2bc6d2",
       "IPY_MODEL_5b130b325efa49f99be9b52994686a0c"
      ],
      "layout": "IPY_MODEL_8f11f250de6448ad87a3363ffcb4e243"
     }
    },
    "be89b15c9d9e48dda2b0819689faa365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d91ce7676d13401fb58641aa4e71602c",
      "placeholder": "​",
      "style": "IPY_MODEL_a614a6f07a7e42baaea992036e16612e",
      "value": " 0/25000 [00:00&lt;?, ? examples/s]"
     }
    },
    "c96784cc747c4dd6b95621289526af8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca846e4a00d5439b89615b29b94dd268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa3b427945774e91bbaf99d8cbdaf10d",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b379d846511c4e17b6ca15c1f6662aae",
      "value": 25000
     }
    },
    "caf08b27977a4c188e92ea34166acfc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb2ca4b8eae746b6990b7a11c90db810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df2756a1e35540abb553129c3b904709",
       "IPY_MODEL_fb9fd2e2a9be4d0d98b6f796cd5f830a",
       "IPY_MODEL_3ebcc18d220745338c4ec8de6127e48e"
      ],
      "layout": "IPY_MODEL_caf08b27977a4c188e92ea34166acfc8"
     }
    },
    "cb85ef2782754246b66058f75317feed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_444ec309e9cc4828b3ae669d28901abc",
      "placeholder": "​",
      "style": "IPY_MODEL_b594fbc34d5d40d4abae1759c81972af",
      "value": "Dl Size...: 100%"
     }
    },
    "d074fc35f4864f55a1e4ebdca6e07530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0a178cf8e0d4bbdb4b521a693ccadbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d704ab225ada4134bf1ce4bb830a6354": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d91ce7676d13401fb58641aa4e71602c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da94d8894e6b4feabcd3d191ffbde635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_825e86dc8b6046a0b60e728a911f99f9",
       "IPY_MODEL_26b2ec9c903547f886effcacfeea8629",
       "IPY_MODEL_e19bf5cbd5ce4377939f080e7aafa25f"
      ],
      "layout": "IPY_MODEL_7df5f5059e5f49c9968599942999833b"
     }
    },
    "df2756a1e35540abb553129c3b904709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f60316df87ba4b64bef0b47f95e8d30f",
      "placeholder": "​",
      "style": "IPY_MODEL_f212087a14d84bd7a5a91d14775a75eb",
      "value": "Dl Completed...: 100%"
     }
    },
    "df8df8420bc342369e233a8a16385efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e236994a32f244b88030884c0495a72e",
       "IPY_MODEL_3aa278af5eb54745a56e4b61af3beac2",
       "IPY_MODEL_fa14c3851c6447c0a28ea94314a832ad"
      ],
      "layout": "IPY_MODEL_29edf5a4a8884ce5a18f71def1ae921b"
     }
    },
    "e014cd8004684dd0bf2513e8dcb9e9f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0716ea457b54fa58b4eef100fab9335": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "e19bf5cbd5ce4377939f080e7aafa25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6cc22bec624493cb11da8a6369b11a8",
      "placeholder": "​",
      "style": "IPY_MODEL_8d682b2a66a14e1ab3345c27b969de1e",
      "value": " 47326/50000 [00:09&lt;00:00, 7169.31 examples/s]"
     }
    },
    "e236994a32f244b88030884c0495a72e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_505a0b8197ab4680a3c51aff901b1569",
      "placeholder": "​",
      "style": "IPY_MODEL_94e07dc24c01434aa319100832e9d21c",
      "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.MK1RO0_1.0.0/imdb_reviews-unsupervised.tfrecord*...:   0%"
     }
    },
    "e8c785ed43d84e4e856476928b2a49e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9b7a63ccc5f462abb99fa20caa65cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec25dadc702141049913b810b7fde1c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f212087a14d84bd7a5a91d14775a75eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3cbd7d3127f4cefb81665e7f7d7c004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b71a01539a0f4b0c89bf87eba3780fcc",
      "placeholder": "​",
      "style": "IPY_MODEL_816a5390057d4567915bf34ad0b54c41",
      "value": "Generating train examples...:  75%"
     }
    },
    "f60316df87ba4b64bef0b47f95e8d30f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa14c3851c6447c0a28ea94314a832ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54eef9cd5ba54636ba508be7c5e65d70",
      "placeholder": "​",
      "style": "IPY_MODEL_2a83a645921d4970b8ed0b4b29f123bc",
      "value": " 0/50000 [00:00&lt;?, ? examples/s]"
     }
    },
    "fa3b427945774e91bbaf99d8cbdaf10d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb9fd2e2a9be4d0d98b6f796cd5f830a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a85a9cfd1b254b7b98ce7a81661b96ed",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec25dadc702141049913b810b7fde1c7",
      "value": 1
     }
    },
    "ff5c2c87b086478295f9aa8ab1b83908": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
