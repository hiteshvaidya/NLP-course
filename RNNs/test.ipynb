{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def generate_sequences(seq_length, padding, vocabulary, \n",
    "                       delimiter, unknown, output_len):\n",
    "    input = []\n",
    "    output = []\n",
    "    for index in range(seq_length):\n",
    "        input.append(np.random.choice(vocabulary))\n",
    "    output = input.copy()\n",
    "    for index in range(padding):\n",
    "        input.append(delimiter)\n",
    "    output_padding = len(input)\n",
    "    for index in range(output_len):\n",
    "        input.append(unknown)\n",
    "    output = output_padding * [unknown] + output[:output_len]\n",
    "    return input, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['a', 'x', 'c', 'r', 'y', 'w', 'b', 't', 'o', '$', ' ']\n",
      "idx_vocab: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Generating 10000 train samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1640.23it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_length = [100, 200, 500, 1000]\n",
    "vocabulary = ['a','x','c','r','y','w','b','t','o']\n",
    "delimiter = '$'\n",
    "unknown = ' '\n",
    "vocabulary.extend([delimiter, unknown])\n",
    "print(f\"vocabulary: {vocabulary}\")\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(vocabulary)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocabulary)}\n",
    "idx_vocab = [char2idx[char] for char in vocabulary]\n",
    "print(f\"idx_vocab: {idx_vocab}\")\n",
    "idx_vocab.remove(char2idx[delimiter])\n",
    "idx_vocab.remove(char2idx[unknown])\n",
    "\n",
    "padding = [10, 20, 50]  # repeat delimiter for how many time steps\n",
    "output_len = [50, 100, 200] # how many time steps to predict\n",
    "batch_size = 32\n",
    "input_size = len(vocabulary)\n",
    "hidden_size = 128\n",
    "n_epochs = 10\n",
    "lr = 0.01\n",
    "n_samples = 10000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "tqdm.write(f\"Generating {n_samples} train samples...\")\n",
    "for index in tqdm(range(n_samples)):\n",
    "    input, output = generate_sequences(seq_length[0], padding=padding[0], vocabulary=idx_vocab, delimiter=char2idx[delimiter], unknown=char2idx[unknown], output_len=output_len[0])\n",
    "    X_train.append(input)\n",
    "    Y_train.append(output)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "pkl.dump((X_train, Y_train), open('copyTask_data_N10000_T100_P10_O50.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 160), (10000, 160))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  3,  7,  4,  6,  2,  6,  7,  4,  3,  7,  7,  2,  5,  4,  1,  7,\n",
       "        5,  1,  4,  0,  5,  8,  0,  2,  6,  3,  8,  2,  4,  2,  6,  4,  8,\n",
       "        6,  1,  3,  8,  1,  8,  4,  1,  3,  6,  7,  2,  0,  3,  1,  7,  3,\n",
       "        1,  5,  5,  3,  5,  1,  1,  3,  7,  6,  8,  7,  4,  1,  4,  7,  8,\n",
       "        8,  0,  8,  6,  8,  7,  0,  7,  7,  2,  0,  7,  2,  2,  0,  4,  6,\n",
       "        8,  6,  8,  7,  1,  0,  6,  6,  7,  4,  2,  7,  5,  2,  0,  9,  9,\n",
       "        9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b', 'r', 't', 'y', 'b', 'c', 'b', 't', 'y', 'r', 't', 't', 'c',\n",
       "       'w', 'y', 'x', 't', 'w', 'x', 'y', 'a', 'w', 'o', 'a', 'c', 'b',\n",
       "       'r', 'o', 'c', 'y', 'c', 'b', 'y', 'o', 'b', 'x', 'r', 'o', 'x',\n",
       "       'o', 'y', 'x', 'r', 'b', 't', 'c', 'a', 'r', 'x', 't', 'r', 'x',\n",
       "       'w', 'w', 'r', 'w', 'x', 'x', 'r', 't', 'b', 'o', 't', 'y', 'x',\n",
       "       'y', 't', 'o', 'o', 'a', 'o', 'b', 'o', 't', 'a', 't', 't', 'c',\n",
       "       'a', 't', 'c', 'c', 'a', 'y', 'b', 'o', 'b', 'o', 't', 'x', 'a',\n",
       "       'b', 'b', 't', 'y', 'c', 't', 'w', 'c', 'a', '$', '$', '$', '$',\n",
       "       '$', '$', '$', '$', '$', '$', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' '], dtype='<U1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.array([idx2char[i] for i in X_train[0]])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10,  6,  3,  7,  4,  6,  2,  6,  7,  4,\n",
       "        3,  7,  7,  2,  5,  4,  1,  7,  5,  1,  4,  0,  5,  8,  0,  2,  6,\n",
       "        3,  8,  2,  4,  2,  6,  4,  8,  6,  1,  3,  8,  1,  8,  4,  1,  3,\n",
       "        6,  7,  2,  0,  3,  1,  7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ',\n",
       "       ' ', ' ', ' ', ' ', ' ', ' ', 'b', 'r', 't', 'y', 'b', 'c', 'b',\n",
       "       't', 'y', 'r', 't', 't', 'c', 'w', 'y', 'x', 't', 'w', 'x', 'y',\n",
       "       'a', 'w', 'o', 'a', 'c', 'b', 'r', 'o', 'c', 'y', 'c', 'b', 'y',\n",
       "       'o', 'b', 'x', 'r', 'o', 'x', 'o', 'y', 'x', 'r', 'b', 't', 'c',\n",
       "       'a', 'r', 'x', 't'], dtype='<U1')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.array([idx2char[i] for i in Y_train[0]])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len vocab: 11\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlen vocab: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocabulary)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m y_one_hot = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_one_hot.shape)\n",
      "\u001b[31mTypeError\u001b[39m: one_hot(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"len vocab: {len(vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 160, 11])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = F.one_hot(torch.tensor(Y_train), num_classes=len(vocabulary))\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(10)\n",
    "a[-5:] = 1\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
